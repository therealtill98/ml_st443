{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T14:16:30.743153Z",
     "start_time": "2025-11-11T14:16:30.691171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Imports\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "print(\"All Imports OK\")"
   ],
   "id": "1dce2e8dd968861",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Imports OK\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T17:21:36.524857Z",
     "start_time": "2025-11-10T17:21:36.522935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Setup directories to save outputs\n",
    "os.makedirs(\"confusion_matrices\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)"
   ],
   "id": "25450fa3ac71692a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-10T17:21:41.464877Z",
     "start_time": "2025-11-10T17:21:36.529459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load dataframe\n",
    "df = pd.read_csv('../../data/data-1.csv.gz')"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T17:21:41.484340Z",
     "start_time": "2025-11-10T17:21:41.481628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set random state for reproducibility\n",
    "RANDOM_STATE = 10291999"
   ],
   "id": "497de8bddfdc72fa",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T14:18:34.597909Z",
     "start_time": "2025-11-11T14:18:31.160420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create X and Y columns and define train/test split\n",
    "\n",
    "# Feature engineering\n",
    "feature_cols = [col for col in df.columns if col.startswith('Band_')] + ['p_x', 'p_y']\n",
    "X = df[feature_cols]\n",
    "y = pd.factorize(df['land_type'])[0]  # Convert to numeric\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Store class labels for proper evaluation\n",
    "class_names = np.array(sorted(df['land_type'].unique()))  # for reference\n",
    "classes = np.array(range(len(class_names)))  # for sklearn functions\n",
    "print(f\"Classes ({len(classes)}): {classes}\")"
   ],
   "id": "31e22d0752c980fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes (8): [0 1 2 3 4 5 6 7]\n",
      "Numeric labels range: 0 to 7\n",
      "Class counts: [38424 50657 38900 24495 26314 15153 10128 11533]\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T17:21:43.888947Z",
     "start_time": "2025-11-10T17:21:42.556209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Pre-processing: Scaling and PCA setup\n",
    "\n",
    "# Scaling for models that need it (LDA, Logistic, QDA, k-NN, SVM)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Unscaled for tree-based models (Random Forest, GBDT)\n",
    "X_train_tree = X_train.copy()\n",
    "X_test_tree = X_test.copy()\n",
    "\n",
    "# PCA setup\n",
    "pca = PCA(n_components=10, random_state=RANDOM_STATE)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)"
   ],
   "id": "606df9d9bdc27981",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T17:21:43.979351Z",
     "start_time": "2025-11-10T17:21:43.942193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define functions for model evaluation and saving outputs\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name, variant=\"raw\", class_labels=None, auto_save=True, results_file=\"all_results.csv\"):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : fitted sklearn model\n",
    "        The trained model to evaluate\n",
    "    X_test : array-like\n",
    "        Test features\n",
    "    y_test : array-like\n",
    "        True test labels\n",
    "    model_name : str\n",
    "        Name of the model (e.g., \"LDA\", \"Random Forest\")\n",
    "    variant : str\n",
    "        Either \"raw\" or \"pca10\" to track preprocessing variant\n",
    "    class_labels : array-like, optional\n",
    "        Class labels in order. If None, will be inferred from y_test\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary with all required metrics\n",
    "    \"\"\"\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Get class probabilities if available\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_proba = model.predict_proba(X_test)\n",
    "    else:\n",
    "        raise ValueError(f\"Model {model_name} doesn't support predict_proba - needed for AUC calculation\")\n",
    "\n",
    "    # Get class labels if not provided\n",
    "    if class_labels is None:\n",
    "        class_labels = np.unique(y_test)\n",
    "\n",
    "    # 1. Overall accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # 2. Overall misclassification error rate\n",
    "    misclass_error = 1.0 - accuracy\n",
    "\n",
    "    # 3. Average balanced accuracy (one-vs-rest for each class)\n",
    "    balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # 4. Average F1 score (macro-averaged)\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # 5. Average AUC (one-vs-rest for each class, then averaged)\n",
    "    # For multiclass: treat each class as positive vs all others, then average\n",
    "    auc_scores = []\n",
    "    y_test_array = np.array(y_test)\n",
    "\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        # Create binary labels: current class vs all others\n",
    "        y_binary = (y_test_array == class_label).astype(int)\n",
    "\n",
    "        # Get probability for this class\n",
    "        if len(y_proba.shape) > 1 and y_proba.shape[1] > i:\n",
    "            y_prob_class = y_proba[:, i]\n",
    "        else:\n",
    "            continue  # Skip if class not in predictions\n",
    "\n",
    "        # Calculate AUC for this class\n",
    "        try:\n",
    "            auc_class = roc_auc_score(y_binary, y_prob_class)\n",
    "            auc_scores.append(auc_class)\n",
    "        except ValueError:\n",
    "            # Skip if class has no positive samples in test set\n",
    "            continue\n",
    "\n",
    "    auc_macro = np.mean(auc_scores) if auc_scores else np.nan\n",
    "\n",
    "    # 6. Confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred, labels=class_labels)\n",
    "\n",
    "    # Compile results\n",
    "    results = {\n",
    "        'Model': model_name,\n",
    "        'Variant': variant,\n",
    "        'Accuracy': accuracy,\n",
    "        'Misclass_Error': misclass_error,\n",
    "        'Balanced_Accuracy': balanced_acc,\n",
    "        'F1_Macro': f1_macro,\n",
    "        'AUC_Macro': auc_macro,\n",
    "        'Confusion_Matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "    if auto_save:\n",
    "        df = pd.DataFrame([{k: v for k, v in results.items() if k != 'Confusion_Matrix'}])\n",
    "        results_path = os.path.join(\"results\", results_file)\n",
    "        df.to_csv(results_path, mode='a', header=not os.path.exists(results_path), index=False)\n",
    "        save_confusion_matrix(results)\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"{model_name} ({variant}):\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Misclass Error: {misclass_error:.4f}\")\n",
    "    print(f\"  Balanced Accuracy: {balanced_acc:.4f}\")\n",
    "    print(f\"  F1 (macro): {f1_macro:.4f}\")\n",
    "    print(f\"  AUC (macro): {auc_macro:.4f}\")\n",
    "    print()\n",
    "\n",
    "    return results\n",
    "\n",
    "def save_results_table(results_list, filename=\"results_summary.csv\"):\n",
    "    \"\"\"\n",
    "    Save all model results to a CSV table.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    results_list : list of dicts\n",
    "        List of result dictionaries from evaluate_model()\n",
    "    filename : str\n",
    "        Output CSV filename\n",
    "    \"\"\"\n",
    "    # Convert to DataFrame (excluding confusion matrices for the summary table)\n",
    "    df_results = []\n",
    "    for result in results_list:\n",
    "        row = {k: v for k, v in result.items() if k != 'Confusion_Matrix'}\n",
    "        df_results.append(row)\n",
    "\n",
    "    df = pd.DataFrame(df_results)\n",
    "\n",
    "    # Sort by variant and then by balanced accuracy\n",
    "    df = df.sort_values(['Variant', 'Balanced_Accuracy'], ascending=[True, False])\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Results saved to {filename}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def save_confusion_matrix(result, output_dir=\"confusion_matrices\"):\n",
    "    \"\"\"\n",
    "    Save individual confusion matrix to CSV.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    result : dict\n",
    "        Result dictionary from evaluate_model()\n",
    "    output_dir : str\n",
    "        Directory to save confusion matrices\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    model_name = result['Model'].replace(' ', '_')\n",
    "    variant = result['Variant']\n",
    "\n",
    "    # Convert confusion matrix to DataFrame with class labels\n",
    "    conf_matrix = result['Confusion_Matrix']\n",
    "\n",
    "    # You'll need to pass class_labels to this function or store them in results\n",
    "    # For now, using generic labels\n",
    "    n_classes = conf_matrix.shape[0]\n",
    "    class_names = [f\"Class_{i}\" for i in range(n_classes)]\n",
    "\n",
    "    conf_df = pd.DataFrame(\n",
    "        conf_matrix,\n",
    "        index=[f\"True_{name}\" for name in class_names],\n",
    "        columns=[f\"Pred_{name}\" for name in class_names]\n",
    "    )\n",
    "\n",
    "    filename = f\"{output_dir}/confmat_{model_name}_{variant}.csv\"\n",
    "    conf_df.to_csv(filename)\n",
    "    print(f\"Confusion matrix saved to {filename}\")"
   ],
   "id": "43fc1b3c1050a577",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# After training your model:\\nresults = evaluate_model(lda, X_test_scaled, y_test, \"LDA\", variant=\"raw\")\\n\\n# Collect all results:\\nall_results = []\\nall_results.append(results)\\n\\n# Save summary table:\\nsave_results_table(all_results, \"model_comparison.csv\")\\n\\n# Save confusion matrix:\\nsave_confusion_matrix(results)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T17:23:21.499389Z",
     "start_time": "2025-11-10T17:21:44.003333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# LINEAR MODELS (Raw Features) - Need Scaling\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Training Linear Models (Raw Features)...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Linear Discriminant Analysis (LDA)\n",
    "print(\"Training LDA...\")\n",
    "lda = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "lda.fit(X_train_scaled, y_train)\n",
    "evaluate_model(lda, X_test_scaled, y_test, \"LDA\", variant=\"raw\", class_labels=classes)\n",
    "\n",
    "# 2. Logistic Regression\n",
    "print(\"Training Logistic Regression...\")\n",
    "logreg = LogisticRegression(\n",
    "    solver='lbfgs',\n",
    "    max_iter=1000,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "evaluate_model(logreg, X_test_scaled, y_test, \"Logistic Regression\", variant=\"raw\", class_labels=classes)\n",
    "\n",
    "# 3. Quadratic Discriminant Analysis (QDA)\n",
    "print(\"Training QDA...\")\n",
    "qda = QuadraticDiscriminantAnalysis(reg_param=0.1)  # regularization for stability\n",
    "qda.fit(X_train_scaled, y_train)\n",
    "evaluate_model(qda, X_test_scaled, y_test, \"QDA\", variant=\"raw\", class_labels=classes)\n",
    "\n",
    "print(\"Linear models (raw features) completed!\")\n",
    "print(\"=\" * 50)"
   ],
   "id": "37bd5c6662942c05",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Linear Models (Raw Features)...\n",
      "==================================================\n",
      "Training LDA...\n",
      "Confusion matrix saved to confusion_matrices/confmat_LDA_raw.csv\n",
      "LDA (raw):\n",
      "  Accuracy: 0.8627\n",
      "  Misclass Error: 0.1373\n",
      "  Balanced Accuracy: 0.8597\n",
      "  F1 (macro): 0.8611\n",
      "  AUC (macro): 0.9910\n",
      "\n",
      "Training Logistic Regression...\n",
      "Confusion matrix saved to confusion_matrices/confmat_Logistic_Regression_raw.csv\n",
      "Logistic Regression (raw):\n",
      "  Accuracy: 0.9924\n",
      "  Misclass Error: 0.0076\n",
      "  Balanced Accuracy: 0.9917\n",
      "  F1 (macro): 0.9917\n",
      "  AUC (macro): 1.0000\n",
      "\n",
      "Training QDA...\n",
      "Confusion matrix saved to confusion_matrices/confmat_QDA_raw.csv\n",
      "QDA (raw):\n",
      "  Accuracy: 0.9532\n",
      "  Misclass Error: 0.0468\n",
      "  Balanced Accuracy: 0.9491\n",
      "  F1 (macro): 0.9492\n",
      "  AUC (macro): 0.9981\n",
      "\n",
      "Linear models (raw features) completed!\n",
      "==================================================\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T17:51:49.328986Z",
     "start_time": "2025-11-10T17:23:21.567654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# DISTANCE-BASED MODELS (Raw Features) - Need Scaling\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Training Distance-Based Models (Raw Features)...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 4. k-Nearest Neighbors (k-NN)\n",
    "print(\"Training k-NN...\")\n",
    "knn = KNeighborsClassifier(\n",
    "    n_neighbors=5,\n",
    "    weights='distance',  # weight by inverse distance\n",
    "    n_jobs=-1  # use all cores for faster computation\n",
    ")\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "evaluate_model(knn, X_test_scaled, y_test, \"k-NN\", variant=\"raw\", class_labels=classes)\n",
    "\n",
    "# 4. RBF SVM\n",
    "print(\"Training RBF SVM...\")\n",
    "# Set environment variables for better BLAS performance\n",
    "os.environ['OMP_NUM_THREADS'] = '8'  # Use all CPU cores\n",
    "os.environ['MKL_NUM_THREADS'] = '8'  # Intel MKL optimization\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '8'  # OpenBLAS optimization\n",
    "\n",
    "# 2. Memory cleanup\n",
    "gc.collect()  # Free up any unused memory\n",
    "\n",
    "# 3. Data type optimization (float32 uses half the memory of float64)\n",
    "X_train_scaled_opt = X_train_scaled.astype(np.float32)\n",
    "X_test_scaled_opt = X_test_scaled.astype(np.float32)\n",
    "\n",
    "# 4. RBF SVM\n",
    "svm_optimized = SVC(\n",
    "    kernel='rbf',                # Keep RBF kernel as required\n",
    "    C=1.0,                      # Standard C parameter\n",
    "    gamma='scale',              # Standard gamma parameter\n",
    "    probability=True,           # Needed for predict_proba\n",
    "    cache_size=4500,            # Increase cache\n",
    "    shrinking=True,             # Enable shrinking heuristic for faster convergence\n",
    "    tol=1e-3,                   # Standard tolerance (explicit for clarity)\n",
    "    max_iter=10000,                # Iteration limit\n",
    "    random_state=RANDOM_STATE   # Reproducibility\n",
    ")\n",
    "svm_optimized.fit(X_train_scaled_opt, y_train)\n",
    "evaluate_model(svm_optimized, X_test_scaled_opt, y_test, \"SVM\", variant=\"raw\", class_labels=classes)\n",
    "\n",
    "# 5. Clean up optimized arrays if memory is tight\n",
    "del X_train_scaled_opt, X_test_scaled_opt\n",
    "gc.collect()\n",
    "\n",
    "print(\"Distance-based models (raw features) completed!\")\n",
    "print(\"=\" * 50)"
   ],
   "id": "f858b61f258e6082",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Distance-Based Models (Raw Features)...\n",
      "==================================================\n",
      "Training k-NN...\n",
      "Confusion matrix saved to confusion_matrices/confmat_k-NN_raw.csv\n",
      "k-NN (raw):\n",
      "  Accuracy: 0.9693\n",
      "  Misclass Error: 0.0307\n",
      "  Balanced Accuracy: 0.9693\n",
      "  F1 (macro): 0.9697\n",
      "  AUC (macro): 0.9984\n",
      "\n",
      "Training optimized RBF SVM...\n",
      "Converting data to float32 for memory efficiency...\n",
      "Memory reduction: 289.5MB -> 144.8MB\n",
      "Training optimized RBF SVM...\n",
      "Starting SVM training (optimized)...\n",
      "SVM training completed! Running evaluation...\n",
      "Confusion matrix saved to confusion_matrices/confmat_SVM_raw.csv\n",
      "SVM (raw):\n",
      "  Accuracy: 0.9899\n",
      "  Misclass Error: 0.0101\n",
      "  Balanced Accuracy: 0.9895\n",
      "  F1 (macro): 0.9897\n",
      "  AUC (macro): 0.9999\n",
      "\n",
      "SVM optimization completed!\n",
      "==================================================\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T00:48:36.694725Z",
     "start_time": "2025-11-10T23:31:02.698593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# TREE-BASED MODELS (Raw Features) - No Scaling Needed\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Training Tree-Based Models (Raw Features)...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 6. Random Forest\n",
    "print(\"Training Random Forest...\")\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,  # more trees for better performance\n",
    "    max_depth=None,    # let trees grow deep\n",
    "    min_samples_split=2,\n",
    "    n_jobs=-1,  # use all cores\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "rf.fit(X_train_tree, y_train)\n",
    "evaluate_model(rf, X_test_tree, y_test, \"Random Forest\", variant=\"raw\", class_labels=classes)\n",
    "\n",
    "# 7. Gradient Boosting Decision Trees (GBDT)\n",
    "print(\"Training GBDT...\")\n",
    "gbdt = GradientBoostingClassifier(\n",
    "    n_estimators=100,  # reduce if too slow\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "gbdt.fit(X_train_tree, y_train)\n",
    "evaluate_model(gbdt, X_test_tree, y_test, \"GBDT\", variant=\"raw\", class_labels=classes)\n",
    "\n",
    "print(\"Tree-based models (raw features) completed!\")\n",
    "print(\"=\" * 50)"
   ],
   "id": "6b9eebef868e0e36",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Tree-Based Models (Raw Features)...\n",
      "==================================================\n",
      "Training Random Forest...\n",
      "Confusion matrix saved to confusion_matrices/confmat_Random_Forest_raw.csv\n",
      "Random Forest (raw):\n",
      "  Accuracy: 0.9843\n",
      "  Misclass Error: 0.0157\n",
      "  Balanced Accuracy: 0.9830\n",
      "  F1 (macro): 0.9829\n",
      "  AUC (macro): 0.9999\n",
      "\n",
      "Training GBDT (this may take longer)...\n",
      "Confusion matrix saved to confusion_matrices/confmat_GBDT_raw.csv\n",
      "GBDT (raw):\n",
      "  Accuracy: 0.9608\n",
      "  Misclass Error: 0.0392\n",
      "  Balanced Accuracy: 0.9631\n",
      "  F1 (macro): 0.9635\n",
      "  AUC (macro): 0.9992\n",
      "\n",
      "Tree-based models (raw features) completed!\n",
      "==================================================\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T00:48:52.475780Z",
     "start_time": "2025-11-11T00:48:36.788638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# LINEAR MODELS (PCA Features) - 10 Components\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Training Linear Models (PCA-10 Features)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# System optimizations\n",
    "os.environ['OMP_NUM_THREADS'] = '8'\n",
    "os.environ['MKL_NUM_THREADS'] = '8'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '8'\n",
    "gc.collect()\n",
    "\n",
    "# Convert PCA data to float32 for memory efficiency\n",
    "X_train_pca_opt = X_train_pca.astype(np.float32)\n",
    "X_test_pca_opt = X_test_pca.astype(np.float32)\n",
    "\n",
    "# 1. LDA + PCA(10)\n",
    "print(\"Training LDA + PCA(10)...\")\n",
    "lda_pca = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "lda_pca.fit(X_train_pca_opt, y_train)\n",
    "evaluate_model(lda_pca, X_test_pca_opt, y_test, \"LDA\", variant=\"pca10\", class_labels=classes)\n",
    "\n",
    "# 2. Logistic Regression + PCA(10)\n",
    "print(\"Training Logistic Regression + PCA(10)...\")\n",
    "logreg_pca = LogisticRegression(\n",
    "    solver='lbfgs',\n",
    "    max_iter=1000,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "logreg_pca.fit(X_train_pca_opt, y_train)\n",
    "evaluate_model(logreg_pca, X_test_pca_opt, y_test, \"Logistic Regression\", variant=\"pca10\", class_labels=classes)\n",
    "\n",
    "# 3. QDA + PCA(10)\n",
    "print(\"Training QDA + PCA(10)...\")\n",
    "qda_pca = QuadraticDiscriminantAnalysis(reg_param=0.1)\n",
    "qda_pca.fit(X_train_pca_opt, y_train)\n",
    "evaluate_model(qda_pca, X_test_pca_opt, y_test, \"QDA\", variant=\"pca10\", class_labels=classes)\n",
    "\n",
    "print(\"Linear models (PCA-10 features) completed!\")\n",
    "print(\"=\" * 50)"
   ],
   "id": "a4bb351fe264bff0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Linear Models (PCA-10 Features) - Optimized...\n",
      "==================================================\n",
      "Converting PCA data to float32...\n",
      "PCA Memory reduction: 13.2MB -> 6.6MB\n",
      "Training LDA + PCA(10)...\n",
      "Confusion matrix saved to confusion_matrices/confmat_LDA_pca10.csv\n",
      "LDA (pca10):\n",
      "  Accuracy: 0.8397\n",
      "  Misclass Error: 0.1603\n",
      "  Balanced Accuracy: 0.8304\n",
      "  F1 (macro): 0.8354\n",
      "  AUC (macro): 0.9883\n",
      "\n",
      "Training Logistic Regression + PCA(10)...\n",
      "Confusion matrix saved to confusion_matrices/confmat_Logistic_Regression_pca10.csv\n",
      "Logistic Regression (pca10):\n",
      "  Accuracy: 0.9905\n",
      "  Misclass Error: 0.0095\n",
      "  Balanced Accuracy: 0.9907\n",
      "  F1 (macro): 0.9907\n",
      "  AUC (macro): 0.9999\n",
      "\n",
      "Training QDA + PCA(10)...\n",
      "Confusion matrix saved to confusion_matrices/confmat_QDA_pca10.csv\n",
      "QDA (pca10):\n",
      "  Accuracy: 0.9531\n",
      "  Misclass Error: 0.0469\n",
      "  Balanced Accuracy: 0.9497\n",
      "  F1 (macro): 0.9492\n",
      "  AUC (macro): 0.9981\n",
      "\n",
      "Linear models (PCA-10 features) completed!\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tilltornieporth/15 Tech/ml_st443/.venv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T10:31:45.124152Z",
     "start_time": "2025-11-11T10:28:31.854975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# DISTANCE-BASED MODELS (PCA Features) - 10 Components\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Training Distance-Based Models (PCA-10 Features) - Optimized...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 4. k-NN + PCA(10)\n",
    "print(\"Training k-NN + PCA(10)...\")\n",
    "knn_pca = KNeighborsClassifier(\n",
    "    n_neighbors=5,\n",
    "    weights='distance',\n",
    "    algorithm='ball_tree',  # Optimized for PCA features\n",
    "    leaf_size=30,\n",
    "    n_jobs=-1\n",
    ")\n",
    "knn_pca.fit(X_train_pca_opt, y_train)\n",
    "evaluate_model(knn_pca, X_test_pca_opt, y_test, \"k-NN\", variant=\"pca10\", class_labels=classes)\n",
    "\n",
    "# 5. SVM + PCA(10)\n",
    "print(\"Training SVM + PCA(10)...\")\n",
    "svm_pca = SVC(\n",
    "    kernel='rbf',\n",
    "    C=1.0,\n",
    "    gamma='scale',\n",
    "    probability=True,\n",
    "    cache_size=2000,\n",
    "    shrinking=True,\n",
    "    tol=1e-3,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "svm_pca.fit(X_train_pca_opt, y_train)\n",
    "evaluate_model(svm_pca, X_test_pca_opt, y_test, \"SVM\", variant=\"pca10\", class_labels=classes)\n",
    "\n",
    "print(\"Distance-based models (PCA-10 features) completed!\")\n",
    "print(\"=\" * 50)"
   ],
   "id": "d22feda98ddab4cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Distance-Based Models (PCA-10 Features) - Optimized...\n",
      "==================================================\n",
      "Training k-NN + PCA(10)...\n",
      "Confusion matrix saved to confusion_matrices/confmat_k-NN_pca10.csv\n",
      "k-NN (pca10):\n",
      "  Accuracy: 0.9704\n",
      "  Misclass Error: 0.0296\n",
      "  Balanced Accuracy: 0.9702\n",
      "  F1 (macro): 0.9706\n",
      "  AUC (macro): 0.9986\n",
      "\n",
      "Training SVM + PCA(10) - Optimized (should be fast)...\n",
      "Confusion matrix saved to confusion_matrices/confmat_SVM_pca10.csv\n",
      "SVM (pca10):\n",
      "  Accuracy: 0.9888\n",
      "  Misclass Error: 0.0112\n",
      "  Balanced Accuracy: 0.9887\n",
      "  F1 (macro): 0.9889\n",
      "  AUC (macro): 0.9999\n",
      "\n",
      "Distance-based models (PCA-10 features) completed!\n",
      "==================================================\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T11:47:50.520488Z",
     "start_time": "2025-11-11T11:47:12.433074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# TREE-BASED MODELS (PCA Features) - 10 Components\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Training Tree-Based Models (PCA-10 Features) - Optimized...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# System optimization for tree models\n",
    "os.environ['OMP_NUM_THREADS'] = '8'\n",
    "gc.collect()\n",
    "\n",
    "# 6. Random Forest + PCA(10)\n",
    "print(\"Training Random Forest + PCA(10)...\")\n",
    "rf_pca = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    max_features='sqrt',\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "rf_pca.fit(X_train_pca_opt, y_train)\n",
    "evaluate_model(rf_pca, X_test_pca_opt, y_test, \"Random Forest\", variant=\"pca10\", class_labels=classes)\n",
    "\n",
    "# 7. GBDT + PCA(10)\n",
    "print(\"Training GBDT + PCA(10)...\")\n",
    "gbdt_pca = GradientBoostingClassifier(\n",
    "    n_estimators=100,       # Should be fast with only 10 features\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    subsample=0.8,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "gbdt_pca.fit(X_train_pca_opt, y_train)\n",
    "evaluate_model(gbdt_pca, X_test_pca_opt, y_test, \"GBDT\", variant=\"pca10\", class_labels=classes)\n",
    "\n",
    "# Clean up optimized PCA arrays\n",
    "del X_train_pca_opt, X_test_pca_opt\n",
    "gc.collect()\n",
    "\n",
    "print(\"Tree-based models (PCA-10 features) completed!\")\n",
    "print(\"All 14 model variants completed!\")\n",
    "print(\"=\" * 50)"
   ],
   "id": "5d23652d0155ff3b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Tree-Based Models (PCA-10 Features) - Optimized...\n",
      "==================================================\n",
      "Using optimized PCA data for tree models...\n",
      "Training Random Forest + PCA(10)...\n",
      "Confusion matrix saved to confusion_matrices/confmat_Random_Forest_pca10.csv\n",
      "Random Forest (pca10):\n",
      "  Accuracy: 0.9773\n",
      "  Misclass Error: 0.0227\n",
      "  Balanced Accuracy: 0.9739\n",
      "  F1 (macro): 0.9741\n",
      "  AUC (macro): 0.9997\n",
      "\n",
      "Training GBDT + PCA(10)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[23], line 38\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTraining GBDT + PCA(10)...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     31\u001B[0m gbdt_pca \u001B[38;5;241m=\u001B[39m GradientBoostingClassifier(\n\u001B[1;32m     32\u001B[0m     n_estimators\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m,       \u001B[38;5;66;03m# Should be fast with only 10 features\u001B[39;00m\n\u001B[1;32m     33\u001B[0m     learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     36\u001B[0m     random_state\u001B[38;5;241m=\u001B[39mRANDOM_STATE\n\u001B[1;32m     37\u001B[0m )\n\u001B[0;32m---> 38\u001B[0m \u001B[43mgbdt_pca\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train_pca_opt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     39\u001B[0m evaluate_model(gbdt_pca, X_test_pca_opt, y_test, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGBDT\u001B[39m\u001B[38;5;124m\"\u001B[39m, variant\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpca10\u001B[39m\u001B[38;5;124m\"\u001B[39m, class_labels\u001B[38;5;241m=\u001B[39mclasses)\n\u001B[1;32m     41\u001B[0m \u001B[38;5;66;03m# Clean up optimized PCA arrays\u001B[39;00m\n",
      "File \u001B[0;32m~/15 Tech/ml_st443/.venv/lib/python3.9/site-packages/sklearn/base.py:1389\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1382\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1384\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1385\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1386\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1387\u001B[0m     )\n\u001B[1;32m   1388\u001B[0m ):\n\u001B[0;32m-> 1389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/15 Tech/ml_st443/.venv/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:787\u001B[0m, in \u001B[0;36mBaseGradientBoosting.fit\u001B[0;34m(self, X, y, sample_weight, monitor)\u001B[0m\n\u001B[1;32m    784\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_resize_state()\n\u001B[1;32m    786\u001B[0m \u001B[38;5;66;03m# fit the boosting stages\u001B[39;00m\n\u001B[0;32m--> 787\u001B[0m n_stages \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_stages\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    788\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    789\u001B[0m \u001B[43m    \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    790\u001B[0m \u001B[43m    \u001B[49m\u001B[43mraw_predictions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    791\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_weight_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    792\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_rng\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    793\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX_val\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    794\u001B[0m \u001B[43m    \u001B[49m\u001B[43my_val\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    795\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_weight_val\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    796\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbegin_at_stage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    797\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmonitor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    798\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    800\u001B[0m \u001B[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001B[39;00m\n\u001B[1;32m    801\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_stages \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimators_\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]:\n",
      "File \u001B[0;32m~/15 Tech/ml_st443/.venv/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:883\u001B[0m, in \u001B[0;36mBaseGradientBoosting._fit_stages\u001B[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001B[0m\n\u001B[1;32m    876\u001B[0m         initial_loss \u001B[38;5;241m=\u001B[39m factor \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_loss(\n\u001B[1;32m    877\u001B[0m             y_true\u001B[38;5;241m=\u001B[39my_oob_masked,\n\u001B[1;32m    878\u001B[0m             raw_prediction\u001B[38;5;241m=\u001B[39mraw_predictions[\u001B[38;5;241m~\u001B[39msample_mask],\n\u001B[1;32m    879\u001B[0m             sample_weight\u001B[38;5;241m=\u001B[39msample_weight_oob_masked,\n\u001B[1;32m    880\u001B[0m         )\n\u001B[1;32m    882\u001B[0m \u001B[38;5;66;03m# fit next stage of trees\u001B[39;00m\n\u001B[0;32m--> 883\u001B[0m raw_predictions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_stage\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    884\u001B[0m \u001B[43m    \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    885\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    886\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    887\u001B[0m \u001B[43m    \u001B[49m\u001B[43mraw_predictions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    888\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    889\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    890\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    891\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX_csc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX_csc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    892\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX_csr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX_csr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    893\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    895\u001B[0m \u001B[38;5;66;03m# track loss\u001B[39;00m\n\u001B[1;32m    896\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m do_oob:\n",
      "File \u001B[0;32m~/15 Tech/ml_st443/.venv/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:495\u001B[0m, in \u001B[0;36mBaseGradientBoosting._fit_stage\u001B[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001B[0m\n\u001B[1;32m    493\u001B[0m \u001B[38;5;66;03m# update tree leaves\u001B[39;00m\n\u001B[1;32m    494\u001B[0m X_for_tree_update \u001B[38;5;241m=\u001B[39m X_csr \u001B[38;5;28;01mif\u001B[39;00m X_csr \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m X\n\u001B[0;32m--> 495\u001B[0m \u001B[43m_update_terminal_regions\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    496\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_loss\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    497\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtree\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtree_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    498\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX_for_tree_update\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    499\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    500\u001B[0m \u001B[43m    \u001B[49m\u001B[43mneg_g_view\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    501\u001B[0m \u001B[43m    \u001B[49m\u001B[43mraw_predictions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    502\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    503\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    504\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearning_rate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    505\u001B[0m \u001B[43m    \u001B[49m\u001B[43mk\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    506\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    508\u001B[0m \u001B[38;5;66;03m# add tree to ensemble\u001B[39;00m\n\u001B[1;32m    509\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimators_[i, k] \u001B[38;5;241m=\u001B[39m tree\n",
      "File \u001B[0;32m~/15 Tech/ml_st443/.venv/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:255\u001B[0m, in \u001B[0;36m_update_terminal_regions\u001B[0;34m(loss, tree, X, y, neg_gradient, raw_prediction, sample_weight, sample_mask, learning_rate, k)\u001B[0m\n\u001B[1;32m    253\u001B[0m y_ \u001B[38;5;241m=\u001B[39m y\u001B[38;5;241m.\u001B[39mtake(indices, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m    254\u001B[0m sw \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m sample_weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m sample_weight[indices]\n\u001B[0;32m--> 255\u001B[0m update \u001B[38;5;241m=\u001B[39m \u001B[43mcompute_update\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindices\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mneg_gradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mraw_prediction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    257\u001B[0m \u001B[38;5;66;03m# TODO: Multiply here by learning rate instead of everywhere else.\u001B[39;00m\n\u001B[1;32m    258\u001B[0m tree\u001B[38;5;241m.\u001B[39mvalue[leaf, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m=\u001B[39m update\n",
      "File \u001B[0;32m~/15 Tech/ml_st443/.venv/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:211\u001B[0m, in \u001B[0;36m_update_terminal_regions.<locals>.compute_update\u001B[0;34m(y_, indices, neg_gradient, raw_prediction, k)\u001B[0m\n\u001B[1;32m    209\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mcompute_update\u001B[39m(y_, indices, neg_gradient, raw_prediction, k):\n\u001B[1;32m    210\u001B[0m     \u001B[38;5;66;03m# we take advantage that: y - prob = neg_gradient\u001B[39;00m\n\u001B[0;32m--> 211\u001B[0m     neg_g \u001B[38;5;241m=\u001B[39m \u001B[43mneg_gradient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtake\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindices\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    212\u001B[0m     prob \u001B[38;5;241m=\u001B[39m y_ \u001B[38;5;241m-\u001B[39m neg_g\n\u001B[1;32m    213\u001B[0m     K \u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mn_classes\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T12:28:29.010944Z",
     "start_time": "2025-11-11T11:53:36.120388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# TASK 1.4: GLACIER ICE BINARY CLASSIFICATION (RAW FEATURES)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TASK 1.4: GLACIER ICE vs ALL OTHER LAND TYPES (RAW FEATURES)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define the positive class for glacier detection\n",
    "glacier_positive = \"snow / ice\"\n",
    "_, original_classes = pd.factorize(df['land_type'])\n",
    "correct_glacier_idx = list(original_classes).index(glacier_positive)\n",
    "\n",
    "# Create binary target arrays using existing train/test split\n",
    "y_train_binary = (y_train == correct_glacier_idx).astype(int)\n",
    "y_test_binary = (y_test == correct_glacier_idx).astype(int)\n",
    "\n",
    "# Check class distribution\n",
    "glacier_train_count = np.sum(y_train_binary)\n",
    "glacier_test_count = np.sum(y_test_binary)\n",
    "total_train = len(y_train_binary)\n",
    "total_test = len(y_test_binary)\n",
    "\n",
    "# Create optimized raw features for binary classification\n",
    "X_train_scaled_opt = X_train_scaled.astype(np.float32)\n",
    "X_test_scaled_opt = X_test_scaled.astype(np.float32)\n",
    "\n",
    "# Prepare unscaled data for tree models\n",
    "X_train_tree_opt = X_train.values.astype(np.float32)\n",
    "X_test_tree_opt = X_test.values.astype(np.float32)\n",
    "\n",
    "# System optimizations\n",
    "os.environ['OMP_NUM_THREADS'] = '8'\n",
    "os.environ['MKL_NUM_THREADS'] = '8'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '8'\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\nTraining classifiers for binary glacier detection (RAW FEATURES)...\")\n",
    "print(f\"Primary metric: F1 score (glacier = positive class)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Store results for comparison\n",
    "glacier_corrected_results = []\n",
    "\n",
    "def evaluate_binary_classifier_corrected(model, X_test, y_test, model_name, use_tree_data=False):\n",
    "    \"\"\"\n",
    "    Evaluate binary classifier focusing on F1 score for glacier detection.\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # F1 score with glacier as positive class\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_test, y_pred, pos_label=1, average='binary'\n",
    "    )\n",
    "\n",
    "    # Overall accuracy for context\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Store results\n",
    "    result = {\n",
    "        'Model': model_name,\n",
    "        'Features': 'Raw',\n",
    "        'F1_Glacier': f1,\n",
    "        'Precision_Glacier': precision,\n",
    "        'Recall_Glacier': recall,\n",
    "        'Accuracy': accuracy\n",
    "    }\n",
    "\n",
    "    glacier_corrected_results.append(result)\n",
    "\n",
    "    # Print results\n",
    "    data_type = \"Tree\" if use_tree_data else \"Scaled\"\n",
    "    print(f\"{model_name} (Raw-{data_type}):\")\n",
    "    print(f\"  F1 (glacier): {f1:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print()\n",
    "\n",
    "    return result\n",
    "\n",
    "# =============================================================================\n",
    "# TRAIN ALL 7 CLASSIFIERS FOR GLACIER DETECTION (RAW FEATURES)\n",
    "# =============================================================================\n",
    "\n",
    "# 1. Linear Discriminant Analysis\n",
    "print(\"1. Training LDA for glacier detection (Raw)...\")\n",
    "lda_binary = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
    "lda_binary.fit(X_train_scaled_opt, y_train_binary)\n",
    "evaluate_binary_classifier_corrected(lda_binary, X_test_scaled_opt, y_test_binary, \"LDA\")\n",
    "\n",
    "# 2. Logistic Regression (with balanced class weights)\n",
    "print(\"2. Training Logistic Regression for glacier detection (Raw-Corrected)...\")\n",
    "logreg_binary = LogisticRegression(\n",
    "    solver='lbfgs',\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced',\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "logreg_binary.fit(X_train_scaled_opt, y_train_binary)\n",
    "evaluate_binary_classifier_corrected(logreg_binary, X_test_scaled_opt, y_test_binary, \"Logistic Regression\")\n",
    "\n",
    "# 3. Quadratic Discriminant Analysis\n",
    "print(\"3. Training QDA for glacier detection (Raw)...\")\n",
    "qda_binary = QuadraticDiscriminantAnalysis(reg_param=0.1)\n",
    "qda_binary.fit(X_train_scaled_opt, y_train_binary)\n",
    "evaluate_binary_classifier_corrected(qda_binary, X_test_scaled_opt, y_test_binary, \"QDA\")\n",
    "\n",
    "# 4. k-Nearest Neighbors\n",
    "print(\"4. Training k-NN for glacier detection (Raw)...\")\n",
    "knn_binary = KNeighborsClassifier(\n",
    "    n_neighbors=5,\n",
    "    weights='distance',\n",
    "    algorithm='ball_tree',\n",
    "    n_jobs=-1\n",
    ")\n",
    "knn_binary.fit(X_train_scaled_opt, y_train_binary)\n",
    "evaluate_binary_classifier_corrected(knn_binary, X_test_scaled_opt, y_test_binary, \"k-NN\")\n",
    "\n",
    "# 5. Support Vector Machine (with balanced class weights and optimizations)\n",
    "print(\"5. Training SVM for glacier detection (Raw)...\")\n",
    "svm_binary = SVC(\n",
    "    kernel='rbf',\n",
    "    C=1.0,\n",
    "    gamma='scale',\n",
    "    class_weight='balanced',\n",
    "    cache_size=4000,\n",
    "    shrinking=True,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "svm_binary.fit(X_train_scaled_opt, y_train_binary)\n",
    "evaluate_binary_classifier_corrected(svm_binary, X_test_scaled_opt, y_test_binary, \"SVM\")\n",
    "\n",
    "# 6. Random Forest (with balanced class weights)\n",
    "print(\"6. Training Random Forest for glacier detection (Raw)...\")\n",
    "rf_binary = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=None,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "rf_binary.fit(X_train_tree_opt, y_train_binary)\n",
    "evaluate_binary_classifier_corrected(rf_binary, X_test_tree_opt, y_test_binary, \"Random Forest\", use_tree_data=True)\n",
    "\n",
    "# 7. Gradient Boosting Decision Trees\n",
    "print(\"7. Training GBDT for glacier detection (Raw)...\")\n",
    "gbdt_binary = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "gbdt_binary.fit(X_train_tree_opt, y_train_binary)\n",
    "evaluate_binary_classifier_corrected(gbdt_binary, X_test_tree_opt, y_test_binary, \"GBDT\", use_tree_data=True)\n",
    "\n",
    "# =============================================================================\n",
    "# RESULTS SUMMARY (RAW FEATURES - CORRECTED)\n",
    "# =============================================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"GLACIER DETECTION RESULTS SUMMARY (RAW FEATURES - CORRECTED)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create results DataFrame\n",
    "glacier_corrected_df = pd.DataFrame(glacier_corrected_results)\n",
    "glacier_corrected_df = glacier_corrected_df.sort_values('F1_Glacier', ascending=False)\n",
    "\n",
    "# Display results\n",
    "print(f\"Ranked by F1 Score (glacier = positive class, CORRECTED Raw features):\")\n",
    "print()\n",
    "for idx, row in glacier_corrected_df.iterrows():\n",
    "    print(f\"{row['Model']:20} | F1: {row['F1_Glacier']:.4f} | Precision: {row['Precision_Glacier']:.4f} | Recall: {row['Recall_Glacier']:.4f}\")\n",
    "\n",
    "# Save results\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "glacier_corrected_df.to_csv(\"results/glacier_binary_corrected_results.csv\", index=False)\n",
    "print(f\"\\nResults saved to results/glacier_binary_corrected_results.csv\")\n",
    "\n",
    "# Identify top 3 performers (matching task requirements)\n",
    "print(f\"\\nTop 3 classifiers for glacier detection (CORRECTED - should match friend's results):\")\n",
    "for i, (idx, row) in enumerate(glacier_corrected_df.head(3).iterrows(), 1):\n",
    "    print(f\"{i}. {row['Model']} (F1: {row['F1_Glacier']:.4f})\")\n",
    "\n",
    "print(\"=\" * 60)"
   ],
   "id": "ed68d64a4d9ccf7e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TASK 1.4: GLACIER ICE vs ALL OTHER LAND TYPES (RAW FEATURES)\n",
      "============================================================\n",
      "Positive class (glacier): 'snow / ice'\n",
      "Correct glacier class index (from factorize): 7\n",
      "\n",
      "CORRECTED Binary Class Distribution:\n",
      "Training set: 9,226 glacier / 163,257 other (5.3% glacier)\n",
      "Test set: 2,307 glacier / 40,814 other (5.4% glacier)\n",
      "Total glacier samples: 11,533 (should match ~11,533 from dataset)\n",
      "\n",
      "Creating optimized raw features for glacier detection...\n",
      "Scaled features shape: (172483, 220)\n",
      "Tree features shape: (172483, 220)\n",
      "\n",
      "Training classifiers for binary glacier detection (RAW FEATURES - CORRECTED)...\n",
      "Primary metric: F1 score (glacier = positive class)\n",
      "============================================================\n",
      "1. Training LDA for glacier detection (Raw-Corrected)...\n",
      "LDA (Raw-Scaled):\n",
      "  F1 (glacier): 0.9363\n",
      "  Precision: 0.9452\n",
      "  Recall: 0.9276\n",
      "  Accuracy: 0.9933\n",
      "\n",
      "2. Training Logistic Regression for glacier detection (Raw-Corrected)...\n",
      "Logistic Regression (Raw-Scaled):\n",
      "  F1 (glacier): 0.9899\n",
      "  Precision: 0.9804\n",
      "  Recall: 0.9996\n",
      "  Accuracy: 0.9989\n",
      "\n",
      "3. Training QDA for glacier detection (Raw-Corrected)...\n",
      "QDA (Raw-Scaled):\n",
      "  F1 (glacier): 0.8095\n",
      "  Precision: 0.6837\n",
      "  Recall: 0.9922\n",
      "  Accuracy: 0.9750\n",
      "\n",
      "4. Training k-NN for glacier detection (Raw-Corrected)...\n",
      "k-NN (Raw-Scaled):\n",
      "  F1 (glacier): 0.9917\n",
      "  Precision: 0.9935\n",
      "  Recall: 0.9900\n",
      "  Accuracy: 0.9991\n",
      "\n",
      "5. Training SVM for glacier detection (Raw-Corrected) - This may take several minutes...\n",
      "SVM (Raw-Scaled):\n",
      "  F1 (glacier): 0.9792\n",
      "  Precision: 0.9596\n",
      "  Recall: 0.9996\n",
      "  Accuracy: 0.9977\n",
      "\n",
      "6. Training Random Forest for glacier detection (Raw-Corrected)...\n",
      "Random Forest (Raw-Tree):\n",
      "  F1 (glacier): 0.9898\n",
      "  Precision: 0.9913\n",
      "  Recall: 0.9883\n",
      "  Accuracy: 0.9989\n",
      "\n",
      "7. Training GBDT for glacier detection (Raw-Corrected)...\n",
      "GBDT (Raw-Tree):\n",
      "  F1 (glacier): 0.9881\n",
      "  Precision: 0.9904\n",
      "  Recall: 0.9857\n",
      "  Accuracy: 0.9987\n",
      "\n",
      "============================================================\n",
      "GLACIER DETECTION RESULTS SUMMARY (RAW FEATURES - CORRECTED)\n",
      "============================================================\n",
      "Ranked by F1 Score (glacier = positive class, CORRECTED Raw features):\n",
      "\n",
      "k-NN                 | F1: 0.9917 | Precision: 0.9935 | Recall: 0.9900\n",
      "Logistic Regression  | F1: 0.9899 | Precision: 0.9804 | Recall: 0.9996\n",
      "Random Forest        | F1: 0.9898 | Precision: 0.9913 | Recall: 0.9883\n",
      "GBDT                 | F1: 0.9881 | Precision: 0.9904 | Recall: 0.9857\n",
      "SVM                  | F1: 0.9792 | Precision: 0.9596 | Recall: 0.9996\n",
      "LDA                  | F1: 0.9363 | Precision: 0.9452 | Recall: 0.9276\n",
      "QDA                  | F1: 0.8095 | Precision: 0.6837 | Recall: 0.9922\n",
      "\n",
      "Results saved to results/glacier_binary_corrected_results.csv\n",
      "\n",
      "Top 3 classifiers for glacier detection (CORRECTED - should match friend's results):\n",
      "1. k-NN (F1: 0.9917)\n",
      "2. Logistic Regression (F1: 0.9899)\n",
      "3. Random Forest (F1: 0.9898)\n",
      "\n",
      "Justification for F1 metric:\n",
      "F1 score is ideal for glacier detection because:\n",
      "- Balances precision (avoiding false glacier detections)\n",
      "- Balances recall (not missing actual glacier pixels)\n",
      "- Handles class imbalance better than accuracy\n",
      "- Directly relevant for climate monitoring applications\n",
      "\n",
      "Note: This corrected version targets the actual 'snow / ice' class\n",
      "(index 7) rather than the wrong class from sorted mapping.\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
